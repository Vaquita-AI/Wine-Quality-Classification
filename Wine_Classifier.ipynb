{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b9ff41",
   "metadata": {},
   "source": [
    "# Wine Quality Classification üç∑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e65885",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6f558",
   "metadata": {},
   "source": [
    "The Wine Quality dataset, is a renowned resource on Kaggle, widely used for data science and machine learning projects. It provides a detailed chemical analysis of red wine samples, encompassing variables like acidity, sugar, sulfates, alcohol, and a subjective quality rating. The dataset reflects the real-world complexity and subjectivity of wine quality assessment. Its class imbalance poses an intriguing challenge for predictive modeling, making it an excellent case study for exploring classification techniques and imbalance handling in machine learning. The notebook solves a subset of the Wine Quality dataset that only has 3 labels (representing poor; 0, medium; 1 and premium; 2 quality). This version is used for better visualization of data distribution and allows for smoother testing of more classifiers. The original dataset can be found on Kaggle: https://www.kaggle.com/datasets/yasserh/wine-quality-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282b99cc",
   "metadata": {},
   "source": [
    "## 2. Importing the necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e5c934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Fixes plt.show() not working\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a5980",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5b79e7",
   "metadata": {},
   "source": [
    "## 3.1 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e9e4b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Snippet of the first 5 instances---\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        1  \n",
      "1      9.8        1  \n",
      "2      9.8        1  \n",
      "3      9.8        1  \n",
      "4      9.4        1  \n",
      "---Label: Number of Samples---\n",
      "1    944\n",
      "2    159\n",
      "0     40\n",
      "Name: quality, dtype: int64\n",
      "---Missing Values---\n",
      "fixed acidity           0\n",
      "volatile acidity        0\n",
      "citric acid             0\n",
      "residual sugar          0\n",
      "chlorides               0\n",
      "free sulfur dioxide     0\n",
      "total sulfur dioxide    0\n",
      "density                 0\n",
      "pH                      0\n",
      "sulphates               0\n",
      "alcohol                 0\n",
      "quality                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Read Dataset\n",
    "df = pd.read_csv('Wine_Test_02.csv')\n",
    "\n",
    "# Get a list of quality classes: [0,1,2]\n",
    "quality_classes = df['quality'].unique()\n",
    "\n",
    "attributes = df.columns[:-1]  # exclude the last column 'quality' from attributes to plot\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"---Snippet of the first 5 instances---\\n\",df.head(),sep=\"\")\n",
    "\n",
    "# Print the number of samples per class\n",
    "print(\"---Label: Number of Samples---\\n\",df['quality'].value_counts(),sep=\"\")\n",
    "\n",
    "# Descriptive statistics\n",
    "#print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"---Missing Values---\\n\",df.isnull().sum(),sep=\"\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc9a6cd",
   "metadata": {},
   "source": [
    "## 3.2 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634a268",
   "metadata": {},
   "source": [
    "## 3.2.1 Plotting the Correlation Matrix between Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bfe42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.figure(figsize=(12, 8))\\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap=\\'coolwarm\\')\\nplt.title(\"Correlation Matrix of Features\")\\nplt.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix heatmap\n",
    "\"\"\"\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix of Features\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# correlation_matrix = df.corr()\n",
    "# print(correlation_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141c4c3",
   "metadata": {},
   "source": [
    "![CorrelationMatrix](https://i.imgur.com/9PWelWL.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97d192",
   "metadata": {},
   "source": [
    "## Analyzing the Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186c4a2",
   "metadata": {},
   "source": [
    "### Fixed Acidity:\n",
    "\n",
    "Has a strong positive correlation with density (0.68) and a strong negative correlation with pH (-0.69), which is expected as acids affect the pH level.\n",
    "It also has a moderate positive correlation with citric acid (0.67), suggesting that wines with higher fixed acidity tend to have more citric acid.\n",
    "\n",
    "### Volatile Acidity:\n",
    "\n",
    "Shows a strong negative correlation with citric acid (-0.54), indicating that wines with higher volatile acidity may have lower levels of citric acid.\n",
    "\n",
    "### Density:\n",
    "\n",
    "Shows a strong negative correlation with alcohol (-0.49). This suggests that wines with higher alcohol content tend to be less dense, which aligns with the fact that alcohol is less dense than water."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b462780",
   "metadata": {},
   "source": [
    "## 3.2.2 Plotting the Histograms of each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29fa3f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor attribute in attributes:\\n    plt.figure(figsize=(10, 4))\\n    for quality in quality_classes:\\n        # Select the rows where the quality matches the current class\\n        subset = df[df['quality'] == quality]\\n        # Plot the histogram\\n        sns.histplot(subset[attribute], kde=False, label=str(quality))\\n    plt.title(f'Histogram of {attribute} by wine quality class')\\n    plt.xlabel(attribute)\\n    plt.ylabel('Frequency')\\n    plt.legend(title='Quality class')\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot histograms for each attribute by quality class SEPERATELY\n",
    "\"\"\"\n",
    "for attribute in attributes:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for quality in quality_classes:\n",
    "        # Select the rows where the quality matches the current class\n",
    "        subset = df[df['quality'] == quality]\n",
    "        # Plot the histogram\n",
    "        sns.histplot(subset[attribute], kde=False, label=str(quality))\n",
    "    plt.title(f'Histogram of {attribute} by wine quality class')\n",
    "    plt.xlabel(attribute)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(title='Quality class')\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a15531",
   "metadata": {},
   "source": [
    "![Histograms](https://i.imgur.com/RqI81tc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c7d198",
   "metadata": {},
   "source": [
    "## Optionally, we can also plot all histograms in one view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c660bf41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, feature in enumerate(df.columns[:-1]):  # exclude the 'quality' column\\n    for quality in sorted(df['quality'].unique()):\\n        subset = df[df['quality'] == quality][feature]\\n        axes[i].hist(subset, bins=bins, alpha=0.5, label=f'Quality {quality}', stacked=True)\\n    axes[i].set_xlabel(feature)\\n    axes[i].set_ylabel('Frequency')\\n    axes[i].legend()\\n\\n# Adjust layout to prevent overlap\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacked histograms in one display\n",
    "bins = 15\n",
    "\n",
    "# Define the layout size based on the number of features\n",
    "num_features = len(df.columns) - 1  # exclude the 'quality' column\n",
    "num_rows = int(np.ceil(num_features / 3))\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=3, figsize=(15, num_rows * 3))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot stacked histograms for each feature\n",
    "\"\"\"\n",
    "for i, feature in enumerate(df.columns[:-1]):  # exclude the 'quality' column\n",
    "    for quality in sorted(df['quality'].unique()):\n",
    "        subset = df[df['quality'] == quality][feature]\n",
    "        axes[i].hist(subset, bins=bins, alpha=0.5, label=f'Quality {quality}', stacked=True)\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ec0b7",
   "metadata": {},
   "source": [
    "![Histograms](https://i.imgur.com/SvUEELs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38499010",
   "metadata": {},
   "source": [
    "## Analyzing the Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9af74",
   "metadata": {},
   "source": [
    "The histograms illustrate the distribution of features in the dataset, segmented by the quality classes (0, 1, and 2). The x-axis represents the feature values, and the y-axis represents the frequency of observations. \n",
    "\n",
    "The distribution of quality classes is highly imbalanced, with the majority class '1' having a predominant number of samples (944), overshadowing class '2' (159 samples) and class '0' (40 samples).\n",
    "\n",
    "This imbalance suggests that a classifier may be biased towards predicting the majority class, leading to high overall accuracy but poor performance for the minority classes. Furthermore, due to the significant overlap between the classes for most attributes, it would suggest that the features are not strong discriminators between the classes, which could lead to a lower performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ac7d57",
   "metadata": {},
   "source": [
    "## 4. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ff4c3",
   "metadata": {},
   "source": [
    "## 4.1 Test Run using Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "713decff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One vs. All Classifier Accuracy: 0.8133971291866029\n",
      "One vs. One Classifier Accuracy: 0.8133971291866029\n"
     ]
    }
   ],
   "source": [
    "# Separate features and labels\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#Can be used as a classifer: SVM, LogReg, RF, KNN\n",
    "# One vs. All Classifier\n",
    "ova_classifier = OneVsRestClassifier(SVC(random_state=22)) #initialize the OVA classifier with a SVM\n",
    "ova_classifier.fit(X_train_scaled, y_train) #fit the scaled training data and their labels\n",
    "y_pred_ova = ova_classifier.predict(X_test_scaled) #test the predictions using test data\n",
    "accuracy_ova = accuracy_score(y_test, y_pred_ova) #compute the accuracy of test data using the test labels\n",
    "print(f'One vs. All Classifier Accuracy: {accuracy_ova}')\n",
    "\n",
    "# One vs. One Classifier (exact steps as before, except OvO is used)\n",
    "ovo_classifier = OneVsOneClassifier(SVC(random_state=11))\n",
    "ovo_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred_ovo = ovo_classifier.predict(X_test_scaled)\n",
    "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
    "print(f'One vs. One Classifier Accuracy: {accuracy_ovo}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100a977",
   "metadata": {},
   "source": [
    "## 4.2 Plotting the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95741fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot confusion matrix for OVA\\nConfusionMatrixDisplay(cm_ova, display_labels=ova_classifier.classes_).plot()\\nplt.title('Confusion Matrix -  OneVsAll SVC')\\nplt.show()\\n\\n# Plot confusion matrix for OVO\\nConfusionMatrixDisplay(cm_ovo, display_labels=ovo_classifier.classes_).plot()\\nplt.title('Confusion Matrix - OneVsOne SVC')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute confusion matrices using predictions from previous step\n",
    "cm_ova = confusion_matrix(y_test, y_pred_ova)\n",
    "cm_ovo = confusion_matrix(y_test, y_pred_ovo)\n",
    "\n",
    "\"\"\"\n",
    "# Plot confusion matrix for OVA\n",
    "ConfusionMatrixDisplay(cm_ova, display_labels=ova_classifier.classes_).plot()\n",
    "plt.title('Confusion Matrix -  OneVsAll SVC')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix for OVO\n",
    "ConfusionMatrixDisplay(cm_ovo, display_labels=ovo_classifier.classes_).plot()\n",
    "plt.title('Confusion Matrix - OneVsOne SVC')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ccd8aa",
   "metadata": {},
   "source": [
    "![ConfusionMatrix](https://i.imgur.com/5Z2lKFE.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d694d76",
   "metadata": {},
   "source": [
    "## 4.3 Computing the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9476847c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsAll Classifier Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.81      1.00      0.90       170\n",
      "           2       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.81       209\n",
      "   macro avg       0.27      0.33      0.30       209\n",
      "weighted avg       0.66      0.81      0.73       209\n",
      "\n",
      "OneVsOne Classifier Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.81      1.00      0.90       170\n",
      "           2       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.81       209\n",
      "   macro avg       0.27      0.33      0.30       209\n",
      "weighted avg       0.66      0.81      0.73       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report for OVA\n",
    "print(\"OneVsAll Classifier Report \")\n",
    "print(classification_report(y_test, y_pred_ova, target_names=ova_classifier.classes_.astype(str), zero_division=0))\n",
    "\n",
    "# Print classification report for OVO\n",
    "print(\"OneVsOne Classifier Report\")\n",
    "print(classification_report(y_test, y_pred_ovo, target_names=ovo_classifier.classes_.astype(str), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bec63",
   "metadata": {},
   "source": [
    "## 4.4 Analysis of Current Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde901a0",
   "metadata": {},
   "source": [
    "Class 0:\n",
    "True Positives (TP): 0 (no. of instances correctly predicted)\n",
    "False Negatives (FN) 0: 8 (instances of class 0 were incorrectly predicted as class 1)\n",
    "There are no instances of class 0 predicted as class 2.\n",
    "\n",
    "Class 1:\n",
    "True Positives (TP): 182 (182 instances were correctly)\n",
    "There are no instances of class 1 incorrectly predicted as class 0 or class 2.\n",
    "\n",
    "Class 2:\n",
    "True Positives (TP): 0 (no instances were correctly predicted as class 2)\n",
    "False Negatives (FN): 39 (39 instances of class 2 were incorrectly predicted as class 1)\n",
    "There are no instances of class 2 predicted as class 0.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "The model did not predict any instances as class 0 or class 2 as expected due to the significant imbalance, and therefore bias towards class 1 clearly exists. All instances of class 0 and class 2 were misclassified as class 1. It is worth noting that OvA and OvO showed no difference in accuracy without using Cross Validation.\n",
    "\n",
    "Back to the drawing board!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2665b4f",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0090f071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Value Counts for each label after SMOTE:\n",
      " 1    683\n",
      "2    683\n",
      "0    683\n",
      "Name: quality, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=22)\n",
    "\n",
    "# Fit SMOTE on the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('New Value Counts for each label after SMOTE:\\n',y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b72200",
   "metadata": {},
   "source": [
    "## 6. Model Selection after Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b2ef2",
   "metadata": {},
   "source": [
    "## 6.1 Random Forest Classifier with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f95ed70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV mean score: 0.8814\n",
      "Test set accuracy: 0.6651\n",
      "[[  0   7   1]\n",
      " [  7 137  26]\n",
      " [  3  26   2]]\n",
      "Random Forest Classifier Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.81      0.81      0.81       170\n",
      "           2       0.07      0.06      0.07        31\n",
      "\n",
      "    accuracy                           0.67       209\n",
      "   macro avg       0.29      0.29      0.29       209\n",
      "weighted avg       0.67      0.67      0.67       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RFC(random_state=4)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(rf_classifier, X_train_scaled, y_train_smote, cv=5)\n",
    "\n",
    "# Output the mean cross-validation score\n",
    "print(f'CV mean score: {cv_scores.mean():.4f}')\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_classifier.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_rfc = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(f'Test set accuracy: {accuracy_score(y_test, y_pred_rfc):.4f}')\n",
    "\n",
    "\n",
    "# Compute confusion matrices\n",
    "cm_rfc = confusion_matrix(y_test, y_pred_rfc)\n",
    "print(cm_rfc)\n",
    "\n",
    "\n",
    "# Plot confusion matrix for RFC\n",
    "\"\"\"\n",
    "ConfusionMatrixDisplay(cm_rfc, display_labels=rf_classifier.classes_).plot()\n",
    "plt.title('Confusion Matrix - Upsampled Data - Random Forest Classifier')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Print classification report for RFC\n",
    "print(\"Random Forest Classifier Report \")\n",
    "print(classification_report(y_test, y_pred_rfc, target_names=rf_classifier.classes_.astype(str), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0625e",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 6.2 Neural Network with Dropout, L2 Regularization and Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97352584",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "52/52 [==============================] - 3s 20ms/step - loss: 1.2110 - accuracy: 0.4570 - val_loss: 1.6811 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.1365 - accuracy: 0.5021 - val_loss: 1.6101 - val_accuracy: 0.0415\n",
      "Epoch 3/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.0856 - accuracy: 0.5412 - val_loss: 1.6329 - val_accuracy: 0.0366\n",
      "Epoch 4/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.0490 - accuracy: 0.5601 - val_loss: 1.6524 - val_accuracy: 0.0707\n",
      "Epoch 5/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 1.0221 - accuracy: 0.5973 - val_loss: 1.6108 - val_accuracy: 0.0951\n",
      "Epoch 6/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9990 - accuracy: 0.5918 - val_loss: 1.5729 - val_accuracy: 0.1659\n",
      "Epoch 7/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9710 - accuracy: 0.6077 - val_loss: 1.6271 - val_accuracy: 0.0902\n",
      "Epoch 8/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9674 - accuracy: 0.6236 - val_loss: 1.5467 - val_accuracy: 0.1122\n",
      "Epoch 9/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9250 - accuracy: 0.6388 - val_loss: 1.5878 - val_accuracy: 0.0878\n",
      "Epoch 10/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.9063 - accuracy: 0.6571 - val_loss: 1.5102 - val_accuracy: 0.1415\n",
      "Epoch 11/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8826 - accuracy: 0.6699 - val_loss: 1.5079 - val_accuracy: 0.1610\n",
      "Epoch 12/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8652 - accuracy: 0.6797 - val_loss: 1.5561 - val_accuracy: 0.0805\n",
      "Epoch 13/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8612 - accuracy: 0.6785 - val_loss: 1.5308 - val_accuracy: 0.1146\n",
      "Epoch 14/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8308 - accuracy: 0.6913 - val_loss: 1.5685 - val_accuracy: 0.1000\n",
      "Epoch 15/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.8173 - accuracy: 0.6894 - val_loss: 1.5528 - val_accuracy: 0.0537\n",
      "Epoch 16/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7917 - accuracy: 0.7120 - val_loss: 1.4444 - val_accuracy: 0.1390\n",
      "Epoch 17/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7800 - accuracy: 0.7175 - val_loss: 1.5183 - val_accuracy: 0.1146\n",
      "Epoch 18/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7778 - accuracy: 0.7023 - val_loss: 1.4798 - val_accuracy: 0.1073\n",
      "Epoch 19/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7513 - accuracy: 0.7212 - val_loss: 1.5162 - val_accuracy: 0.0488\n",
      "Epoch 20/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7598 - accuracy: 0.7193 - val_loss: 1.4113 - val_accuracy: 0.1854\n",
      "Epoch 21/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7398 - accuracy: 0.7206 - val_loss: 1.4654 - val_accuracy: 0.0610\n",
      "Epoch 22/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7058 - accuracy: 0.7468 - val_loss: 1.4802 - val_accuracy: 0.0927\n",
      "Epoch 23/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6999 - accuracy: 0.7517 - val_loss: 1.4695 - val_accuracy: 0.0732\n",
      "Epoch 24/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.7113 - accuracy: 0.7419 - val_loss: 1.3882 - val_accuracy: 0.0829\n",
      "Epoch 25/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6966 - accuracy: 0.7425 - val_loss: 1.4446 - val_accuracy: 0.0585\n",
      "Epoch 26/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6964 - accuracy: 0.7419 - val_loss: 1.3798 - val_accuracy: 0.0780\n",
      "Epoch 27/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6712 - accuracy: 0.7474 - val_loss: 1.4194 - val_accuracy: 0.0488\n",
      "Epoch 28/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6505 - accuracy: 0.7553 - val_loss: 1.3112 - val_accuracy: 0.1683\n",
      "Epoch 29/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6556 - accuracy: 0.7547 - val_loss: 1.3925 - val_accuracy: 0.0756\n",
      "Epoch 30/70\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.6490 - accuracy: 0.7675 - val_loss: 1.3715 - val_accuracy: 0.1171\n",
      "Epoch 31/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6367 - accuracy: 0.7694 - val_loss: 1.3516 - val_accuracy: 0.1122\n",
      "Epoch 32/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6230 - accuracy: 0.7743 - val_loss: 1.4272 - val_accuracy: 0.0537\n",
      "Epoch 33/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6174 - accuracy: 0.7724 - val_loss: 1.3510 - val_accuracy: 0.0634\n",
      "Epoch 34/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6269 - accuracy: 0.7639 - val_loss: 1.3431 - val_accuracy: 0.0537\n",
      "Epoch 35/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6087 - accuracy: 0.7736 - val_loss: 1.3005 - val_accuracy: 0.1537\n",
      "Epoch 36/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.6108 - accuracy: 0.7761 - val_loss: 1.3095 - val_accuracy: 0.1439\n",
      "Epoch 37/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5975 - accuracy: 0.7779 - val_loss: 1.3463 - val_accuracy: 0.1220\n",
      "Epoch 38/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5950 - accuracy: 0.7785 - val_loss: 1.3150 - val_accuracy: 0.1780\n",
      "Epoch 39/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5762 - accuracy: 0.7834 - val_loss: 1.3132 - val_accuracy: 0.0902\n",
      "Epoch 40/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5876 - accuracy: 0.7730 - val_loss: 1.3065 - val_accuracy: 0.1049\n",
      "Epoch 41/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5832 - accuracy: 0.7889 - val_loss: 1.2197 - val_accuracy: 0.2024\n",
      "Epoch 42/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5703 - accuracy: 0.7968 - val_loss: 1.3012 - val_accuracy: 0.1146\n",
      "Epoch 43/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5864 - accuracy: 0.7889 - val_loss: 1.2539 - val_accuracy: 0.1415\n",
      "Epoch 44/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5648 - accuracy: 0.7901 - val_loss: 1.2757 - val_accuracy: 0.1073\n",
      "Epoch 45/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5685 - accuracy: 0.7871 - val_loss: 1.2901 - val_accuracy: 0.1415\n",
      "Epoch 46/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5567 - accuracy: 0.7999 - val_loss: 1.2771 - val_accuracy: 0.1732\n",
      "Epoch 47/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5486 - accuracy: 0.7944 - val_loss: 1.2752 - val_accuracy: 0.2049\n",
      "Epoch 48/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5515 - accuracy: 0.8084 - val_loss: 1.2774 - val_accuracy: 0.2049\n",
      "Epoch 49/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5464 - accuracy: 0.8060 - val_loss: 1.2715 - val_accuracy: 0.1585\n",
      "Epoch 50/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5420 - accuracy: 0.8139 - val_loss: 1.1466 - val_accuracy: 0.2756\n",
      "Epoch 51/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5578 - accuracy: 0.7877 - val_loss: 1.2529 - val_accuracy: 0.1976\n",
      "Epoch 52/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5345 - accuracy: 0.8017 - val_loss: 1.2723 - val_accuracy: 0.1951\n",
      "Epoch 53/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5328 - accuracy: 0.8090 - val_loss: 1.2761 - val_accuracy: 0.1707\n",
      "Epoch 54/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5324 - accuracy: 0.8096 - val_loss: 1.2327 - val_accuracy: 0.2195\n",
      "Epoch 55/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5294 - accuracy: 0.8103 - val_loss: 1.3062 - val_accuracy: 0.1659\n",
      "Epoch 56/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5279 - accuracy: 0.7999 - val_loss: 1.1921 - val_accuracy: 0.2415\n",
      "Epoch 57/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5240 - accuracy: 0.8127 - val_loss: 1.2045 - val_accuracy: 0.2415\n",
      "Epoch 58/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5329 - accuracy: 0.8023 - val_loss: 1.2022 - val_accuracy: 0.2146\n",
      "Epoch 59/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5201 - accuracy: 0.8133 - val_loss: 1.1455 - val_accuracy: 0.2585\n",
      "Epoch 60/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5140 - accuracy: 0.8145 - val_loss: 1.1621 - val_accuracy: 0.2634\n",
      "Epoch 61/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5202 - accuracy: 0.8145 - val_loss: 1.1540 - val_accuracy: 0.2415\n",
      "Epoch 62/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5158 - accuracy: 0.8127 - val_loss: 1.1602 - val_accuracy: 0.2707\n",
      "Epoch 63/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5095 - accuracy: 0.8176 - val_loss: 1.2402 - val_accuracy: 0.2244\n",
      "Epoch 64/70\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.5004 - accuracy: 0.8170 - val_loss: 1.1803 - val_accuracy: 0.2634\n",
      "Epoch 65/70\n",
      "52/52 [==============================] - 0s 9ms/step - loss: 0.4993 - accuracy: 0.8237 - val_loss: 1.1861 - val_accuracy: 0.2829\n",
      "Epoch 66/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5046 - accuracy: 0.8029 - val_loss: 1.1673 - val_accuracy: 0.2683\n",
      "Epoch 67/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.4932 - accuracy: 0.8225 - val_loss: 1.1640 - val_accuracy: 0.2902\n",
      "Epoch 68/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5008 - accuracy: 0.8133 - val_loss: 1.1726 - val_accuracy: 0.2610\n",
      "Epoch 69/70\n",
      "52/52 [==============================] - 0s 8ms/step - loss: 0.5057 - accuracy: 0.8145 - val_loss: 1.1564 - val_accuracy: 0.2951\n",
      "Test set accuracy: 0.6986\n",
      "7/7 [==============================] - 0s 4ms/step\n",
      "Neural Network Classifier Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         8\n",
      "           1       0.81      0.83      0.82       170\n",
      "           2       0.21      0.16      0.18        31\n",
      "\n",
      "    accuracy                           0.70       209\n",
      "   macro avg       0.34      0.33      0.33       209\n",
      "weighted avg       0.69      0.70      0.69       209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model with dropout and L2 regularization\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_smote.shape[1],), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')  \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_smote,\n",
    "    epochs=70,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm_nn = confusion_matrix(y_test, y_pred_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "\"\"\"\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred_labels)\n",
    "plt.title('Confusion Matrix - Neural Network')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Neural Network Classifier Report\")\n",
    "print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b1546c",
   "metadata": {},
   "source": [
    "## 7. Insights and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6b835",
   "metadata": {},
   "source": [
    "## 7.1 Test Accuracy and Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3255bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices side by side\n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ConfusionMatrixDisplay(cm_nn, display_labels=[0,1,2]).plot(ax=axes[0], cmap=plt.cm.Blues)\n",
    "axes[0].set_title('NN Confusion Matrix')\n",
    "\n",
    "ConfusionMatrixDisplay(cm_rfc, display_labels=[0,1,2]).plot(ax=axes[1], cmap=plt.cm.Blues)\n",
    "axes[1].set_title('RFC Confusion Matrix')\n",
    "\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029c8812",
   "metadata": {},
   "source": [
    "![ConfusionMatrix](https://i.imgur.com/BrMK0D8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25edfe0",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "The NN has an accuracy of 64.63%, while the RFC has a higher accuracy of 75.11%. This suggests that the RFC is performing better overall on the test set.\n",
    "\n",
    "### Confusion Matrix: \n",
    "The confusion matrices show the distribution of predictions across the actual classes. Both models struggle with class 0 and class 2, but the RFC seems to make more accurate predictions for the majority class (class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd6412",
   "metadata": {},
   "source": [
    "## 7.2 Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca69833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for F1-scores\n",
    "\"\"\"\n",
    "nn_f1_scores = [0.11, 0.77, 0.30]\n",
    "rfc_f1_scores = [0.11, 0.85, 0.34]\n",
    "\n",
    "index = np.arange(len(quality_classes))\n",
    "bar_width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "nn_bar = ax.bar(index, nn_f1_scores, bar_width, label='NN')\n",
    "rfc_bar = ax.bar(index + bar_width, rfc_f1_scores, bar_width, label='RFC')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.set_title('F1 Score by class and model')\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels([0,1,2])\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6db754",
   "metadata": {},
   "source": [
    "![F1 score](https://i.imgur.com/rig6a6T.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a95355",
   "metadata": {},
   "source": [
    "For class 0 and class 2, both models have relatively low scores, indicating difficulty in correctly predicting these classes. However, for the majority class, the RFC performs better in terms of precision and recall, leading to a higher F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be30d709",
   "metadata": {},
   "source": [
    "## 7.3 Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef5085",
   "metadata": {},
   "source": [
    "The overall accuracy decreased for the Random Forest Classifier after upsampling (79% to 75.11%), but this is not a negative outcome, as it reflects a more truthful representation of the model's predictive capabilities across all classes. Nevertheless, both models, show limited effectiveness in classifying minority classes, likely due to persistent class imbalance issues and also due to the non-informative features, despite attempts to mitigate this with SMOTE.\n",
    "\n",
    "The RFC demonstrates better performance over the Neural Network (NN) in terms of accuracy and F1-score, particularly for the predominant class, suggesting better feature handling and resilience to overfitting, though further exploration of other ensemble techniques like Gradient Boosting or AdaBoost may be feasible.\n",
    "\n",
    "The NN could potentially see improvements with more hyperparameter tuning or increasing the dataset size.\n",
    "\n",
    "Ultimately, while the RFC currently leads in performance, there is room for enhancement in both models through more refined tuning and a more strategic approach to class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07f7de",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3457b8",
   "metadata": {},
   "source": [
    "The project highlights the common challenge of class imbalances in classification tasks, where the RFC's robustness to overfitting and its impressive handling of features have given it an edge over the NN, particularly after upsampling efforts attempted to mitigate class imbalances. The evaluation through precision, recall, and F1-scores underscores the limitations of relying only on accuracy as a performance metric, revealing a more comprehensive picture of the model's performance in imbalanced datasets. While the NN's complexity suggests a need for larger datasets to achieve better generalization, the RFC's ensemble approach demonstrates a strong capacity for working with the available data. This project reinforces the importance of an all-round assessment of model performance, considering the complex balance between various types of errors and their implications in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
